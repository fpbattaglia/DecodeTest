{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Decoding Analysis for Neural Replay\n",
    "\n",
    "Based on **Liu et al. (2019) Cell**: *Human Replay Spontaneously Reorganizes Experience*\n",
    "\n",
    "This notebook demonstrates the multivariate decoding algorithm for detecting sequential replay of neural representations during rest periods.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "1. **Stimulus Decoders**: Train classifiers to recognize neural patterns associated with each stimulus\n",
    "2. **Reactivation Detection**: Apply decoders to rest periods to detect spontaneous reactivations\n",
    "3. **Sequenceness Measure**: Quantify the degree to which reactivations follow a sequential structure\n",
    "4. **Statistical Testing**: Use permutation tests to establish significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Implementation of the Decoding Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateReplayDecoder:\n",
    "    \"\"\"\n",
    "    Multivariate decoding analysis for detecting neural replay sequences\n",
    "    based on Liu et al. (2019) Cell paper.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_states=8, max_lag_ms=600, sampling_rate=100):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        n_states : int\n",
    "            Number of distinct states/stimuli\n",
    "        max_lag_ms : int\n",
    "            Maximum time lag to test in milliseconds\n",
    "        sampling_rate : int\n",
    "            Sampling rate in Hz\n",
    "        \"\"\"\n",
    "        self.n_states = n_states\n",
    "        self.max_lag_ms = max_lag_ms\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.max_lag_samples = int(max_lag_ms * sampling_rate / 1000)\n",
    "        self.classifiers = []\n",
    "        self.scalers = []\n",
    "        \n",
    "    def train_classifiers(self, X_train, y_train, time_point_ms=200, C=1.0):\n",
    "        \"\"\"\n",
    "        Train binary classifiers for each state using lasso logistic regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_train : ndarray, shape (n_trials, n_timepoints, n_sensors)\n",
    "            Training data from functional localizer\n",
    "        y_train : ndarray, shape (n_trials,)\n",
    "            Labels for each trial (0 to n_states-1)\n",
    "        time_point_ms : int\n",
    "            Time point relative to stimulus onset to use for training\n",
    "        C : float\n",
    "            Inverse regularization strength\n",
    "        \"\"\"\n",
    "        time_idx = int(time_point_ms * self.sampling_rate / 1000)\n",
    "        X_at_time = X_train[:, time_idx, :]  # Extract specific time point\n",
    "        \n",
    "        self.classifiers = []\n",
    "        self.scalers = []\n",
    "        \n",
    "        # Train one binary classifier per state\n",
    "        for state in range(self.n_states):\n",
    "            # Create binary labels\n",
    "            y_binary = (y_train == state).astype(int)\n",
    "            \n",
    "            # Standardize features\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(X_at_time)\n",
    "            \n",
    "            # Train L1-regularized logistic regression\n",
    "            clf = LogisticRegression(penalty='l1', C=C, solver='liblinear',\n",
    "                                    max_iter=1000, random_state=42)\n",
    "            clf.fit(X_scaled, y_binary)\n",
    "            \n",
    "            self.classifiers.append(clf)\n",
    "            self.scalers.append(scaler)\n",
    "            \n",
    "        print(f\"Trained {self.n_states} classifiers\")\n",
    "        \n",
    "    def decode_states(self, X_rest):\n",
    "        \"\"\"\n",
    "        Apply trained classifiers to resting-state data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X_rest : ndarray, shape (n_timepoints, n_sensors)\n",
    "            Resting state MEG/EEG data\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        probabilities : ndarray, shape (n_timepoints, n_states)\n",
    "            Reactivation probabilities for each state at each time point\n",
    "        \"\"\"\n",
    "        n_timepoints = X_rest.shape[0]\n",
    "        probabilities = np.zeros((n_timepoints, self.n_states))\n",
    "        \n",
    "        for state in range(self.n_states):\n",
    "            X_scaled = self.scalers[state].transform(X_rest)\n",
    "            probabilities[:, state] = self.classifiers[state].predict_proba(X_scaled)[:, 1]\n",
    "            \n",
    "        return probabilities\n",
    "    \n",
    "    def compute_sequenceness(self, reactivation_probs, transition_matrix, \n",
    "                            time_lags_ms=None, alpha_control=True):\n",
    "        \"\"\"\n",
    "        Compute sequenceness measure using time-lagged regression.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        reactivation_probs : ndarray, shape (n_timepoints, n_states)\n",
    "            State reactivation probabilities from decode_states()\n",
    "        transition_matrix : ndarray, shape (n_states, n_states)\n",
    "            Hypothesized transition structure (1 for transitions, 0 otherwise)\n",
    "        time_lags_ms : array-like, optional\n",
    "            Specific time lags to test in milliseconds\n",
    "        alpha_control : bool\n",
    "            Whether to include nuisance regressors for 10Hz oscillations\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        sequenceness : ndarray\n",
    "            Sequenceness measure at each time lag (forward - backward)\n",
    "        time_lags : ndarray\n",
    "            Time lags tested in milliseconds\n",
    "        \"\"\"\n",
    "        if time_lags_ms is None:\n",
    "            time_lags_ms = np.arange(10, self.max_lag_ms + 1, 10)\n",
    "            \n",
    "        time_lags_samples = (time_lags_ms * self.sampling_rate / 1000).astype(int)\n",
    "        sequenceness_forward = np.zeros(len(time_lags_ms))\n",
    "        sequenceness_backward = np.zeros(len(time_lags_ms))\n",
    "        \n",
    "        Y = reactivation_probs  # Current activations\n",
    "        n_timepoints, n_states = Y.shape\n",
    "        \n",
    "        # Normalize transition matrix\n",
    "        P = transition_matrix / (np.sum(transition_matrix) + 1e-10)\n",
    "        \n",
    "        for lag_idx, lag in enumerate(time_lags_samples):\n",
    "            # Create time-lagged design matrix\n",
    "            X_lag = self._create_lagged_matrix(reactivation_probs, lag, alpha_control)\n",
    "            \n",
    "            # Fit regression for each state\n",
    "            beta_matrix = np.zeros((n_states, n_states))\n",
    "            \n",
    "            for state_i in range(n_states):\n",
    "                # Regression: Y_i = X(Δt) * β\n",
    "                if lag > 0:\n",
    "                    y_i = Y[lag:, state_i]\n",
    "                    X_reg = X_lag[:len(y_i)]\n",
    "                else:\n",
    "                    y_i = Y[:lag, state_i]\n",
    "                    X_reg = X_lag[:len(y_i)]\n",
    "                \n",
    "                # Ordinary least squares\n",
    "                try:\n",
    "                    beta = np.linalg.lstsq(X_reg, y_i, rcond=None)[0]\n",
    "                    beta_matrix[state_i, :] = beta[:n_states]\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            # Project onto transition matrix (Frobenius inner product)\n",
    "            sequenceness_forward[lag_idx] = np.sum(beta_matrix * P)\n",
    "            \n",
    "            # Backward direction (transpose)\n",
    "            sequenceness_backward[lag_idx] = np.sum(beta_matrix * P.T)\n",
    "        \n",
    "        # Sequenceness = forward - backward\n",
    "        sequenceness = sequenceness_forward - sequenceness_backward\n",
    "        \n",
    "        return sequenceness, time_lags_ms\n",
    "    \n",
    "    def _create_lagged_matrix(self, reactivation_probs, lag, alpha_control=True):\n",
    "        \"\"\"\n",
    "        Create time-lagged predictor matrix with optional alpha confounds.\n",
    "        \"\"\"\n",
    "        n_timepoints, n_states = reactivation_probs.shape\n",
    "        \n",
    "        # Basic lagged matrix\n",
    "        if lag > 0:\n",
    "            X_lag = reactivation_probs[:-lag, :]\n",
    "        else:\n",
    "            X_lag = reactivation_probs[-lag:, :]\n",
    "        \n",
    "        if alpha_control:\n",
    "            # Add confound regressors at Δt+100ms, Δt+200ms, ... up to Δt+600ms\n",
    "            confounds = []\n",
    "            for extra_lag_ms in range(100, 700, 100):\n",
    "                extra_lag_samples = int(extra_lag_ms * self.sampling_rate / 1000)\n",
    "                total_lag = lag + extra_lag_samples\n",
    "                \n",
    "                if total_lag < n_timepoints:\n",
    "                    if total_lag > 0:\n",
    "                        confound = reactivation_probs[:-total_lag, :]\n",
    "                    else:\n",
    "                        confound = reactivation_probs[-total_lag:, :]\n",
    "                    \n",
    "                    # Match length\n",
    "                    min_len = min(X_lag.shape[0], confound.shape[0])\n",
    "                    confounds.append(confound[:min_len, :])\n",
    "            \n",
    "            if confounds:\n",
    "                # Concatenate confounds\n",
    "                min_len = min([X_lag.shape[0]] + [c.shape[0] for c in confounds])\n",
    "                X_lag = X_lag[:min_len, :]\n",
    "                confounds = [c[:min_len, :] for c in confounds]\n",
    "                X_lag = np.hstack([X_lag] + confounds)\n",
    "        \n",
    "        # Add constant term\n",
    "        X_lag = np.hstack([X_lag, np.ones((X_lag.shape[0], 1))])\n",
    "        \n",
    "        return X_lag\n",
    "    \n",
    "    def permutation_test(self, reactivation_probs, transition_matrix, \n",
    "                        n_permutations=1000, time_lags_ms=None):\n",
    "        \"\"\"\n",
    "        Statistical testing via permutation of stimulus labels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        reactivation_probs : ndarray\n",
    "            State reactivation probabilities\n",
    "        transition_matrix : ndarray\n",
    "            Hypothesized transition structure\n",
    "        n_permutations : int\n",
    "            Number of permutations\n",
    "        time_lags_ms : array-like, optional\n",
    "            Time lags to test\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        p_values : ndarray\n",
    "            P-values at each time lag\n",
    "        threshold : float\n",
    "            Significance threshold (corrected for multiple comparisons)\n",
    "        true_seq : ndarray\n",
    "            True sequenceness values\n",
    "        \"\"\"\n",
    "        # Compute true sequenceness\n",
    "        true_seq, time_lags = self.compute_sequenceness(\n",
    "            reactivation_probs, transition_matrix, time_lags_ms\n",
    "        )\n",
    "        \n",
    "        # Permutation distribution\n",
    "        perm_max_abs = np.zeros(n_permutations)\n",
    "        \n",
    "        for perm in range(n_permutations):\n",
    "            # Permute transition matrix (shuffle rows and columns together)\n",
    "            perm_indices = np.random.permutation(self.n_states)\n",
    "            P_perm = transition_matrix[perm_indices, :][:, perm_indices]\n",
    "            \n",
    "            # Compute sequenceness with permuted matrix\n",
    "            perm_seq, _ = self.compute_sequenceness(\n",
    "                reactivation_probs, P_perm, time_lags_ms\n",
    "            )\n",
    "            \n",
    "            # Store maximum absolute value across lags\n",
    "            perm_max_abs[perm] = np.max(np.abs(perm_seq))\n",
    "        \n",
    "        # Threshold at 95th percentile\n",
    "        threshold = np.percentile(perm_max_abs, 95)\n",
    "        \n",
    "        # Compute p-values\n",
    "        p_values = np.array([\n",
    "            np.mean(perm_max_abs >= np.abs(seq_val))\n",
    "            for seq_val in true_seq\n",
    "        ])\n",
    "        \n",
    "        return p_values, threshold, true_seq\n",
    "\n",
    "print(\"MultivariateReplayDecoder class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Example 1 - Basic Simulation with Forward Replay\n",
    "\n",
    "We'll simulate a simple scenario:\n",
    "- 4 states: A, B, C, D\n",
    "- Sequence: A → B → C → D\n",
    "- Replay lag: 40ms (as in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Parameters\n",
    "n_trials = 200\n",
    "n_timepoints_per_trial = 50  # 500ms at 100Hz\n",
    "n_sensors = 100\n",
    "n_states = 4\n",
    "\n",
    "print(\"Creating simulated training data...\")\n",
    "print(f\"  - {n_trials} trials\")\n",
    "print(f\"  - {n_states} states (A, B, C, D)\")\n",
    "print(f\"  - {n_sensors} sensors\")\n",
    "print(f\"  - {n_timepoints_per_trial} timepoints per trial (500ms)\")\n",
    "\n",
    "# Simulate training data (functional localizer)\n",
    "X_train = np.random.randn(n_trials, n_timepoints_per_trial, n_sensors) * 0.5\n",
    "y_train = np.random.randint(0, n_states, n_trials)\n",
    "\n",
    "# Add signal at 200ms for each state\n",
    "time_idx = 20  # 200ms\n",
    "for trial in range(n_trials):\n",
    "    state = y_train[trial]\n",
    "    # Add state-specific pattern to specific sensors\n",
    "    X_train[trial, time_idx, state*10:(state+1)*10] += 2.0\n",
    "\n",
    "print(\"\\nTraining data created!\")\n",
    "print(f\"Shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "state_names = ['A', 'B', 'C', 'D']\n",
    "for state in range(n_states):\n",
    "    # Get trials for this state\n",
    "    state_trials = X_train[y_train == state]\n",
    "    \n",
    "    # Average across trials and plot\n",
    "    avg_activity = state_trials.mean(axis=0)  # (timepoints, sensors)\n",
    "    \n",
    "    im = axes[state].imshow(avg_activity.T, aspect='auto', cmap='RdBu_r', \n",
    "                            vmin=-0.5, vmax=0.5, origin='lower')\n",
    "    axes[state].set_xlabel('Time (ms)', fontsize=10)\n",
    "    axes[state].set_ylabel('Sensor', fontsize=10)\n",
    "    axes[state].set_title(f'State {state_names[state]} - Average Activity', fontsize=12, fontweight='bold')\n",
    "    axes[state].axvline(x=20, color='yellow', linestyle='--', linewidth=2, label='Decoding timepoint')\n",
    "    axes[state].legend(loc='upper right', fontsize=8)\n",
    "    \n",
    "    # Set x-axis labels in milliseconds\n",
    "    xticks = axes[state].get_xticks()\n",
    "    axes[state].set_xticklabels([int(x*10) for x in xticks])\n",
    "\n",
    "plt.colorbar(im, ax=axes, label='Activity (a.u.)', fraction=0.046, pad=0.04)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_data_heatmap.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training data visualization saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train decoder\n",
    "print(\"Initializing decoder...\")\n",
    "decoder = MultivariateReplayDecoder(n_states=n_states, max_lag_ms=600, sampling_rate=100)\n",
    "\n",
    "print(\"\\nTraining classifiers...\")\n",
    "decoder.train_classifiers(X_train, y_train, time_point_ms=200, C=1.0)\n",
    "\n",
    "# Evaluate decoder performance with cross-validation\n",
    "print(\"\\nEvaluating decoder performance (cross-validation)...\")\n",
    "time_idx = 20\n",
    "X_at_time = X_train[:, time_idx, :]\n",
    "\n",
    "for state in range(n_states):\n",
    "    y_binary = (y_train == state).astype(int)\n",
    "    scores = cross_val_score(decoder.classifiers[state], \n",
    "                            decoder.scalers[state].transform(X_at_time), \n",
    "                            y_binary, cv=5, scoring='roc_auc')\n",
    "    print(f\"  State {state_names[state]}: AUC = {scores.mean():.3f} ± {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate resting state data with embedded replay\n",
    "print(\"Simulating resting state with replay sequence A→B→C→D...\")\n",
    "n_rest_timepoints = 3000  # 30 seconds at 100Hz\n",
    "X_rest = np.random.randn(n_rest_timepoints, n_sensors) * 0.3\n",
    "\n",
    "# Embed sequence A→B→C→D at 40ms lag, at multiple time points\n",
    "lag_samples = 4  # 40ms at 100Hz\n",
    "sequence = [0, 1, 2, 3]  # A→B→C→D\n",
    "replay_times = []\n",
    "\n",
    "for start_time in range(100, n_rest_timepoints - 100, 200):\n",
    "    replay_times.append(start_time)\n",
    "    for step, state in enumerate(sequence):\n",
    "        time_point = start_time + step * lag_samples\n",
    "        if time_point < n_rest_timepoints:\n",
    "            # Add signal to state-specific sensors\n",
    "            X_rest[time_point, state*10:(state+1)*10] += 1.5\n",
    "\n",
    "print(f\"  Embedded {len(replay_times)} replay events\")\n",
    "print(f\"  Replay lag: 40ms between states\")\n",
    "print(f\"  Replay interval: ~200ms between events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode states from resting data\n",
    "print(\"\\nDecoding states from resting data...\")\n",
    "reactivation_probs = decoder.decode_states(X_rest)\n",
    "print(f\"  Reactivation matrix shape: {reactivation_probs.shape}\")\n",
    "print(f\"  Time resolution: 10ms\")\n",
    "\n",
    "# Visualize reactivations\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "\n",
    "# Plot first 5 seconds\n",
    "time_window = slice(0, 500)\n",
    "time_ms = np.arange(500) * 10\n",
    "\n",
    "for state in range(n_states):\n",
    "    ax.plot(time_ms, reactivation_probs[time_window, state], \n",
    "           label=f'State {state_names[state]}', linewidth=1.5, alpha=0.8)\n",
    "\n",
    "# Mark embedded replay events\n",
    "for rt in replay_times:\n",
    "    if rt < 500:\n",
    "        ax.axvline(x=rt*10, color='red', linestyle='--', alpha=0.3, linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Time (ms)', fontsize=12)\n",
    "ax.set_ylabel('Reactivation Probability', fontsize=12)\n",
    "ax.set_title('State Reactivations During Rest (First 5 seconds)', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', ncol=4)\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('reactivation_timeseries.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Reactivation visualization saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transition matrix for A→B→C→D\n",
    "transition_matrix = np.array([\n",
    "    [0, 1, 0, 0],  # A → B\n",
    "    [0, 0, 1, 0],  # B → C\n",
    "    [0, 0, 0, 1],  # C → D\n",
    "    [0, 0, 0, 0]   # D → nothing\n",
    "])\n",
    "\n",
    "print(\"Transition matrix (A→B→C→D):\")\n",
    "print(transition_matrix)\n",
    "\n",
    "# Visualize transition matrix\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "im = ax.imshow(transition_matrix, cmap='Blues', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(n_states))\n",
    "ax.set_yticks(range(n_states))\n",
    "ax.set_xticklabels(state_names)\n",
    "ax.set_yticklabels(state_names)\n",
    "ax.set_xlabel('To State', fontsize=12)\n",
    "ax.set_ylabel('From State', fontsize=12)\n",
    "ax.set_title('Hypothesized Transition Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(n_states):\n",
    "    for j in range(n_states):\n",
    "        text = ax.text(j, i, int(transition_matrix[i, j]),\n",
    "                      ha=\"center\", va=\"center\", color=\"black\", fontsize=16)\n",
    "\n",
    "plt.colorbar(im, ax=ax, label='Transition')\n",
    "plt.tight_layout()\n",
    "plt.savefig('transition_matrix.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sequenceness\n",
    "print(\"Computing sequenceness...\")\n",
    "sequenceness, time_lags = decoder.compute_sequenceness(\n",
    "    reactivation_probs, transition_matrix, alpha_control=True\n",
    ")\n",
    "\n",
    "# Find peak\n",
    "peak_idx = np.argmax(np.abs(sequenceness))\n",
    "peak_lag = time_lags[peak_idx]\n",
    "peak_value = sequenceness[peak_idx]\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Peak sequenceness: {peak_value:.4f}\")\n",
    "print(f\"  Peak lag: {peak_lag}ms\")\n",
    "print(f\"  Expected lag: 40ms (embedded in data)\")\n",
    "print(f\"  Direction: {'Forward' if peak_value > 0 else 'Backward'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run permutation test\n",
    "print(\"\\nRunning permutation test (500 permutations)...\")\n",
    "print(\"This may take a minute...\")\n",
    "\n",
    "p_values, threshold, true_seq = decoder.permutation_test(\n",
    "    reactivation_probs, transition_matrix, n_permutations=500\n",
    ")\n",
    "\n",
    "significant_lags = time_lags[p_values < 0.05]\n",
    "print(f\"\\nStatistical Results:\")\n",
    "print(f\"  Significance threshold: {threshold:.4f}\")\n",
    "print(f\"  Number of significant lags: {len(significant_lags)}\")\n",
    "if len(significant_lags) > 0:\n",
    "    print(f\"  Significant time lags: {significant_lags}ms\")\n",
    "    print(f\"  Peak p-value: {p_values[peak_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sequenceness results\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Plot 1: Sequenceness\n",
    "ax1.plot(time_lags, sequenceness, 'b-', linewidth=2.5, label='Sequenceness')\n",
    "ax1.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax1.axhline(y=threshold, color='r', linestyle='--', linewidth=2, label=f'Threshold (p<0.05)')\n",
    "ax1.axhline(y=-threshold, color='r', linestyle='--', linewidth=2)\n",
    "ax1.axvline(x=40, color='green', linestyle=':', linewidth=2, alpha=0.7, label='True lag (40ms)')\n",
    "ax1.fill_between(time_lags, 0, sequenceness, where=(sequenceness > threshold), \n",
    "                 alpha=0.3, color='blue', label='Significant')\n",
    "ax1.set_xlabel('Time Lag (ms)', fontsize=12)\n",
    "ax1.set_ylabel('Sequenceness\\n(Forward - Backward)', fontsize=12)\n",
    "ax1.set_title('Example 1: Forward Replay Detection (A→B→C→D)', fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: P-values\n",
    "ax2.plot(time_lags, p_values, 'purple', linewidth=2)\n",
    "ax2.axhline(y=0.05, color='r', linestyle='--', linewidth=2, label='p=0.05')\n",
    "ax2.fill_between(time_lags, 0, p_values, where=(p_values < 0.05), \n",
    "                alpha=0.3, color='purple')\n",
    "ax2.set_xlabel('Time Lag (ms)', fontsize=12)\n",
    "ax2.set_ylabel('P-value', fontsize=12)\n",
    "ax2.set_title('Statistical Significance', fontsize=12)\n",
    "ax2.set_yscale('log')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sequenceness_example1.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSequenceness analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Example 2 - Reverse Replay After Reward\n",
    "\n",
    "The paper showed that replay reverses direction after reward. Let's simulate this phenomenon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate resting state with REVERSE replay (D→C→B→A)\n",
    "print(\"Simulating resting state with REVERSE replay (D→C→B→A)...\")\n",
    "X_rest_reverse = np.random.randn(n_rest_timepoints, n_sensors) * 0.3\n",
    "\n",
    "# Embed REVERSE sequence D→C→B→A at 40ms lag\n",
    "reverse_sequence = [3, 2, 1, 0]  # D→C→B→A\n",
    "replay_times_reverse = []\n",
    "\n",
    "for start_time in range(100, n_rest_timepoints - 100, 200):\n",
    "    replay_times_reverse.append(start_time)\n",
    "    for step, state in enumerate(reverse_sequence):\n",
    "        time_point = start_time + step * lag_samples\n",
    "        if time_point < n_rest_timepoints:\n",
    "            X_rest_reverse[time_point, state*10:(state+1)*10] += 1.5\n",
    "\n",
    "print(f\"  Embedded {len(replay_times_reverse)} REVERSE replay events\")\n",
    "print(f\"  Replay lag: 40ms between states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode reverse replay\n",
    "print(\"\\nDecoding reverse replay...\")\n",
    "reactivation_probs_reverse = decoder.decode_states(X_rest_reverse)\n",
    "\n",
    "# Compute sequenceness for reverse replay\n",
    "print(\"Computing sequenceness for reverse replay...\")\n",
    "sequenceness_reverse, _ = decoder.compute_sequenceness(\n",
    "    reactivation_probs_reverse, transition_matrix, alpha_control=True\n",
    ")\n",
    "\n",
    "# Find peak\n",
    "peak_idx_reverse = np.argmax(np.abs(sequenceness_reverse))\n",
    "peak_lag_reverse = time_lags[peak_idx_reverse]\n",
    "peak_value_reverse = sequenceness_reverse[peak_idx_reverse]\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Peak sequenceness: {peak_value_reverse:.4f}\")\n",
    "print(f\"  Peak lag: {peak_lag_reverse}ms\")\n",
    "print(f\"  Direction: {'Forward' if peak_value_reverse > 0 else 'Backward (Reverse)'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare forward and reverse replay\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Forward replay\n",
    "axes[0].plot(time_lags, sequenceness, 'b-', linewidth=2.5)\n",
    "axes[0].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0].axhline(y=threshold, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].axhline(y=-threshold, color='r', linestyle='--', linewidth=2)\n",
    "axes[0].axvline(x=40, color='green', linestyle=':', linewidth=2, alpha=0.7)\n",
    "axes[0].fill_between(time_lags, 0, sequenceness, where=(sequenceness > 0), \n",
    "                     alpha=0.3, color='blue')\n",
    "axes[0].set_xlabel('Time Lag (ms)', fontsize=12)\n",
    "axes[0].set_ylabel('Sequenceness', fontsize=12)\n",
    "axes[0].set_title('Forward Replay\\n(Before Reward: A→B→C→D)', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].text(0.05, 0.95, 'Forward\\n(positive)', transform=axes[0].transAxes,\n",
    "            fontsize=11, verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "\n",
    "# Reverse replay\n",
    "axes[1].plot(time_lags, sequenceness_reverse, 'r-', linewidth=2.5)\n",
    "axes[1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[1].axhline(y=threshold, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].axhline(y=-threshold, color='r', linestyle='--', linewidth=2)\n",
    "axes[1].axvline(x=40, color='green', linestyle=':', linewidth=2, alpha=0.7)\n",
    "axes[1].fill_between(time_lags, 0, sequenceness_reverse, where=(sequenceness_reverse < 0), \n",
    "                     alpha=0.3, color='red')\n",
    "axes[1].set_xlabel('Time Lag (ms)', fontsize=12)\n",
    "axes[1].set_ylabel('Sequenceness', fontsize=12)\n",
    "axes[1].set_title('Reverse Replay\\n(After Reward: D→C→B→A)', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].text(0.05, 0.05, 'Backward\\n(negative)', transform=axes[1].transAxes,\n",
    "            fontsize=11, verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='lightcoral', alpha=0.5))\n",
    "\n",
    "plt.suptitle('Example 2: Direction Reversal After Reward', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('sequenceness_example2_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nComparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Example 3 - Multiple Sequences\n",
    "\n",
    "Real experiments often have multiple possible sequences. Let's test with two interleaved sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 8 states for two sequences\n",
    "n_states_multi = 8\n",
    "decoder_multi = MultivariateReplayDecoder(n_states=n_states_multi, max_lag_ms=600)\n",
    "\n",
    "# Create training data for 8 states\n",
    "print(\"Creating training data for 8 states (2 sequences)...\")\n",
    "X_train_multi = np.random.randn(400, n_timepoints_per_trial, n_sensors) * 0.5\n",
    "y_train_multi = np.random.randint(0, n_states_multi, 400)\n",
    "\n",
    "# Add signal for each state\n",
    "for trial in range(400):\n",
    "    state = y_train_multi[trial]\n",
    "    X_train_multi[trial, time_idx, state*12:(state+1)*12] += 2.0\n",
    "\n",
    "# Train decoder\n",
    "print(\"Training decoder for 8 states...\")\n",
    "decoder_multi.train_classifiers(X_train_multi, y_train_multi, time_point_ms=200)\n",
    "\n",
    "# Define two sequences: Seq1 (0→1→2→3) and Seq2 (4→5→6→7)\n",
    "transition_matrix_seq1 = np.zeros((8, 8))\n",
    "transition_matrix_seq1[0, 1] = 1  # 0→1\n",
    "transition_matrix_seq1[1, 2] = 1  # 1→2\n",
    "transition_matrix_seq1[2, 3] = 1  # 2→3\n",
    "\n",
    "transition_matrix_seq2 = np.zeros((8, 8))\n",
    "transition_matrix_seq2[4, 5] = 1  # 4→5\n",
    "transition_matrix_seq2[5, 6] = 1  # 5→6\n",
    "transition_matrix_seq2[6, 7] = 1  # 6→7\n",
    "\n",
    "print(\"\\nSequence 1: States 0→1→2→3\")\n",
    "print(\"Sequence 2: States 4→5→6→7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate rest with ONLY Sequence 1 replay\n",
    "print(\"\\nSimulating rest with Sequence 1 replay only...\")\n",
    "X_rest_multi = np.random.randn(n_rest_timepoints, n_sensors) * 0.3\n",
    "\n",
    "# Embed Sequence 1: 0→1→2→3\n",
    "seq1 = [0, 1, 2, 3]\n",
    "for start_time in range(100, n_rest_timepoints - 100, 200):\n",
    "    for step, state in enumerate(seq1):\n",
    "        time_point = start_time + step * lag_samples\n",
    "        if time_point < n_rest_timepoints:\n",
    "            X_rest_multi[time_point, state*12:(state+1)*12] += 1.5\n",
    "\n",
    "# Decode\n",
    "print(\"Decoding...\")\n",
    "reactivation_probs_multi = decoder_multi.decode_states(X_rest_multi)\n",
    "\n",
    "# Compute sequenceness for both sequences\n",
    "print(\"\\nComputing sequenceness for both sequences...\")\n",
    "seq1_sequenceness, _ = decoder_multi.compute_sequenceness(\n",
    "    reactivation_probs_multi, transition_matrix_seq1\n",
    ")\n",
    "seq2_sequenceness, _ = decoder_multi.compute_sequenceness(\n",
    "    reactivation_probs_multi, transition_matrix_seq2\n",
    ")\n",
    "\n",
    "# Compare peaks\n",
    "peak1 = np.max(np.abs(seq1_sequenceness))\n",
    "peak2 = np.max(np.abs(seq2_sequenceness))\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Sequence 1 peak: {peak1:.4f}\")\n",
    "print(f\"  Sequence 2 peak: {peak2:.4f}\")\n",
    "print(f\"  Ratio (Seq1/Seq2): {peak1/peak2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize both sequences\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(time_lags, seq1_sequenceness, 'b-', linewidth=2.5, label='Sequence 1 (0→1→2→3)', alpha=0.8)\n",
    "ax.plot(time_lags, seq2_sequenceness, 'orange', linewidth=2.5, label='Sequence 2 (4→5→6→7)', alpha=0.8)\n",
    "ax.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=40, color='green', linestyle=':', linewidth=2, alpha=0.7, label='True lag (40ms)')\n",
    "\n",
    "ax.set_xlabel('Time Lag (ms)', fontsize=12)\n",
    "ax.set_ylabel('Sequenceness', fontsize=12)\n",
    "ax.set_title('Example 3: Sequence-Specific Replay\\n(Only Sequence 1 was embedded in data)', \n",
    "            fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='upper right', fontsize=11)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add annotation\n",
    "ax.text(0.98, 0.95, f'Sequence 1 replays at 40ms\\nSequence 2 shows no replay',\n",
    "       transform=ax.transAxes, fontsize=11, verticalalignment='top', horizontalalignment='right',\n",
    "       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sequenceness_example3_multi.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMulti-sequence analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Summary and Key Findings\n",
    "\n",
    "This notebook demonstrated the multivariate decoding algorithm from Liu et al. (2019):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary visualization\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = fig.add_gridspec(3, 2, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# 1. Training phase\n",
    "ax1 = fig.add_subplot(gs[0, :])\n",
    "ax1.text(0.5, 0.5, 'Step 1: Train Classifiers\\n\\n' +\n",
    "        '• Record neural activity during stimulus presentations\\n' +\n",
    "        '• Train binary classifiers (one per stimulus)\\n' +\n",
    "        '• Use L1-regularized logistic regression\\n' +\n",
    "        '• Focus on specific time point (200ms post-stimulus)',\n",
    "        transform=ax1.transAxes, fontsize=12, verticalalignment='center',\n",
    "        horizontalalignment='center', bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.7))\n",
    "ax1.axis('off')\n",
    "ax1.set_title('Algorithm Overview', fontsize=16, fontweight='bold', pad=20)\n",
    "\n",
    "# 2. Decoding phase\n",
    "ax2 = fig.add_subplot(gs[1, 0])\n",
    "ax2.text(0.5, 0.5, 'Step 2: Decode Rest Activity\\n\\n' +\n",
    "        '• Apply classifiers to rest data\\n' +\n",
    "        '• Generate reactivation probabilities\\n' +\n",
    "        '• Create time series for each state',\n",
    "        transform=ax2.transAxes, fontsize=11, verticalalignment='center',\n",
    "        horizontalalignment='center', bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.7))\n",
    "ax2.axis('off')\n",
    "\n",
    "# 3. Sequenceness computation\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "ax3.text(0.5, 0.5, 'Step 3: Compute Sequenceness\\n\\n' +\n",
    "        '• Time-lagged regression\\n' +\n",
    "        '• Test multiple time lags\\n' +\n",
    "        '• Control for 10Hz oscillations\\n' +\n",
    "        '• Forward - Backward measure',\n",
    "        transform=ax3.transAxes, fontsize=11, verticalalignment='center',\n",
    "        horizontalalignment='center', bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))\n",
    "ax3.axis('off')\n",
    "\n",
    "# 4. Key findings visualization\n",
    "ax4 = fig.add_subplot(gs[2, :])\n",
    "findings_text = (\n",
    "    'KEY FINDINGS:\\n\\n'\n",
    "    '1. FORWARD REPLAY: Sequences play forward during learning/exploration (A→B→C→D)\\n'\n",
    "    f'   Example 1 peak: {peak_value:.3f} at {peak_lag}ms\\n\\n'\n",
    "    '2. REVERSE REPLAY: Sequences reverse after reward (D→C→B→A)\\n'\n",
    "    f'   Example 2 peak: {peak_value_reverse:.3f} at {peak_lag_reverse}ms\\n\\n'\n",
    "    '3. SEQUENCE-SPECIFIC: Only experienced sequences show replay\\n'\n",
    "    f'   Example 3: Seq1/Seq2 ratio = {peak1/peak2:.2f}\\n\\n'\n",
    "    '4. TIME-COMPRESSED: Replay occurs at ~40-50ms intervals (faster than experience)\\n\\n'\n",
    "    '5. STATISTICAL VALIDATION: Permutation tests confirm significance'\n",
    ")\n",
    "ax4.text(0.05, 0.95, findings_text, transform=ax4.transAxes, fontsize=11,\n",
    "        verticalalignment='top', fontfamily='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='lavender', alpha=0.8))\n",
    "ax4.axis('off')\n",
    "\n",
    "plt.suptitle('Multivariate Decoding Analysis - Summary', fontsize=18, fontweight='bold', y=0.98)\n",
    "plt.savefig('analysis_summary.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nGenerated files:\")\n",
    "print(\"  • training_data_heatmap.png\")\n",
    "print(\"  • reactivation_timeseries.png\")\n",
    "print(\"  • transition_matrix.png\")\n",
    "print(\"  • sequenceness_example1.png\")\n",
    "print(\"  • sequenceness_example2_comparison.png\")\n",
    "print(\"  • sequenceness_example3_multi.png\")\n",
    "print(\"  • analysis_summary.png\")\n",
    "print(\"\\nThis implementation demonstrates the core algorithm from:\")\n",
    "print(\"Liu et al. (2019) Cell: Human Replay Spontaneously Reorganizes Experience\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To apply this to real data:\n",
    "\n",
    "1. **Load your MEG/EEG data**: Replace simulated data with real recordings\n",
    "2. **Adjust parameters**: \n",
    "   - Sampling rate\n",
    "   - Number of sensors\n",
    "   - Regularization strength (C parameter)\n",
    "   - Time windows\n",
    "3. **Preprocess data**:\n",
    "   - Filter (high-pass at 0.5 Hz)\n",
    "   - Artifact rejection\n",
    "   - Baseline correction\n",
    "4. **Define your sequences**: Based on experimental design\n",
    "5. **Interpret results**: Consider:\n",
    "   - Behavioral relevance\n",
    "   - Task structure\n",
    "   - Individual differences\n",
    "\n",
    "---\n",
    "\n",
    "**Citation:**\n",
    "Liu, Y., Dolan, R. J., Kurth-Nelson, Z., & Behrens, T. E. (2019). \n",
    "Human replay spontaneously reorganizes experience. *Cell*, 178(3), 640-652."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
